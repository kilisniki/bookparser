- смотреть и сравнивать со словарем из скайенга или любой другой где уже точно есть отмеченные слова которые ты знаешь
- половина слов, 5000, встречаются в тексте всего один раз
- нужно оптимизировать "пачки" учения слов
- разделять и проводить анализ не по всей книге, а по главам
- или страницам даже.
- устанавливать лимиты на количество неизвестных слов на страницу

# inst and run

- python -m venv myenv  
- source myenv/bin/activate 
- pip install -r requirements.txt
- python kn_main.py LordoftheFlies.txt my_vocabulary.xlsx


# TODO что хочется
- парсинг других форматов книг
- парсинг глав вместо всей книги
- запись слов в БД которые я знаю
- фильтрация слов которые я знаю
- запись слов на обучение (очередь)
- бесплатная БД монги (хорошо ли это? или лучше бесплатный vercel и т.д.)
- значение на оригинале и на русском языке тянуть из словарей
- частота использования
- отделять слова для сохранения в словарь от примеров для пользователя. то есть для каждой книги, а возможно даже главы нужно добавлять свой список примеров
- приводить ПОДХОДЯЩИЕ синонимы слова под данный контекст

# TODO что нужно
- составить список слов которые знаю: тексты из английского, конспекты и эссе
- фильтровать по словам которые знаю

- Моя цель: быстрее и комфортнее читать тексты на английском, меньше отвлекаться на перевод во время текста.
- Как плюс: аудио формат чего угодно + генерация всего на устройстве а не облаке. Даже если на это будет требоваться время.

- Ждем openai apple реализации, возможно все будет сильно проще.

# Как начать:
1. Первую главу парсим
2. Фильтруем из слов которые точно знаю
3. Учим
4. Читаем переводим

Идея: можно делать примерно то же самое для русский -> английский...
